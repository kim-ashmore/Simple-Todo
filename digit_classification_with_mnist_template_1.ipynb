{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_classification_with_mnist_template-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim-ashmore/Simple-Todo/blob/master/digit_classification_with_mnist_template_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fh0_xV0rO3xq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recognizing Handwritten Digits using Deep Learning\n",
        "\n",
        "This notebook is one of many deep learning jupyter notebooks that are designed to get you started with machine learning. Specifically, this notebook will walk you through the end-to-end learning of a simple Convolutional Neural Network on the popular MNIST dataset. The code in this notebook has been adapted from Keras Tutorials by Francois Chollet (https://github.com/fchollet/keras-resources) and, of course, StackOverflow discussions, Medium articles and blogs.\n",
        "\n",
        "**Task**: Given an image of a handwritten digit, predict what digit is in the image\n",
        "\n",
        "**Features**: Images of handwritten digits (pixel values)\n",
        "\n",
        "**Target**: Digit in the image\n"
      ]
    },
    {
      "metadata": {
        "id": "-6cdCWgePucT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 0. Import modules and libraries\n",
        "\n",
        "The following code block imports all the packages that are needed to successfully run this notebook. Note that, Google Colaboratory (Colab) provides support for Python 2.7 and 3.6 along with GPU acceleration. Within the Google Colaboratory environment, machine learning and deep learning toolkits are pre-installed on their cloud infrastructure. Specifically, packages such as Tensorflow, Scikit-Learn, Matplotlib are readily available with no additional configurations and hence, can be directly imported. You can read more about the features and benefits of using Colab here - https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "However, for packages that are not pre-installed, you may have to install them manually by following the steps outlined here - https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb "
      ]
    },
    {
      "metadata": {
        "id": "JrS1MNaIaSvl",
        "colab_type": "code",
        "outputId": "250b421d-51ad-4b46-ce68-dda1ae9c169e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qRsEBmLvvfbR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Load the data\n",
        "\n",
        "Most machine learning toolkits/libraries provide few out-of-the-box toy datasets to get you started. The load_data( ) method, pre-defined in Keras, helps in this regard by loading any of such toy datasets. For other customized datasets, you can refer to the source code for load_data( ) and design your own helper function.\n",
        "\n",
        "Reference: https://www.tensorflow.org/code/stable/tensorflow/python/keras/datasets/mnist.py"
      ]
    },
    {
      "metadata": {
        "id": "tCWXrPfVbRlg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bCRV5Igcnwc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. For Your Information!\n",
        "\n",
        "## a. Data Statistics\n",
        "\n",
        "This chunk of code gives information about the statistics of the train and test sets that was read in the last step.  The *shape* property returns the current shape of the array. More simply, *shape* gives the dimension information of the n-dimensional Numpy array.\n",
        "\n",
        "Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html"
      ]
    },
    {
      "metadata": {
        "id": "Rjdf-zxzu9HV",
        "colab_type": "code",
        "outputId": "7f385309-4c57-469e-807d-57a293636793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of images in training = \" + str(x_train.shape[0]))\n",
        "print(\"x_train Examples Loaded = \" + str(x_train.shape))\n",
        "print(\"y_train Examples Loaded = \" + str(y_train.shape))\n",
        "\n",
        "print(\"Number of images in test = \" + str(x_test.shape[0]))\n",
        "print(\"x_test Examples Loaded = \" + str(x_test.shape))\n",
        "print(\"y_test Examples Loaded = \" + str(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in training = 60000\n",
            "x_train Examples Loaded = (60000, 28, 28)\n",
            "y_train Examples Loaded = (60000,)\n",
            "Number of images in test = 10000\n",
            "x_test Examples Loaded = (10000, 28, 28)\n",
            "y_test Examples Loaded = (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "czFsr7uynuUg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## b. Data Visualization\n",
        "\n",
        "The following code snippet can be used to visualize a few images in the MNIST dataset using the ***matplotlib*** visualization module. The *subplot( )* method is used when we would want to plot more than two plots within the same figure. *imshow( )* draws the image of a given plot. Defining these subplots and drawing the image of these plots does not display them when the code is executed. In order to display these plots, we need to use *show( )*.\n",
        "\n",
        "*   *show( )* displays the figure and should not be called unless you want to display the plots you have plotted. Whereas, *imshow( )* only draws an image on the figure. Hence, invoking *show( )* without drawing anything is meaningless.\n",
        "*   When drawing an image on the figure, if there is no figure created previously, then *imshow( )* creates a figure before drawing. Alternatively, if you want to explictly create a new figure, you can also use *plt.figure( )* and then draw your images on this figure via *imshow( )*.\n",
        "\n",
        "The image data that you want to plot and the colormap that should be used to display the image are the two basic parameters to the *imshow( )* method. For more parameters that are available, please refer to https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html"
      ]
    },
    {
      "metadata": {
        "id": "HxW-LCA4b2bj",
        "colab_type": "code",
        "outputId": "3ac31601-35e8-4cec-b917-974de34e94db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "plt.subplot(221)\n",
        "plt.imshow(x_train[100], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(x_train[250], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(x_train[500], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(x_train[750], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgsfETnn/6777W3Z1JQjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5lbFqqGT33Fl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Prepare image data\n",
        "\n",
        "## a. Reshaping the data\n",
        "\n",
        "Before we can use the MNIST data to train a Convolutional Neural Network, we need to reshape the images appropriately. Two-dimensional convolutional layers in Keras require pixel values with the dimensions as *[pixels][width][height]*. In case of RGB images, the pixel dimension would be 3 for the red, green and blue components. Since MNIST consists of images where the pixel values are gray scale, the pixel dimension will be set to 1.\n",
        "\n",
        "The *image_data_format* parameter defines how the backend treats the data dimensions when working with multi-dimensional convolution layers. In other words, this parameter specifies the ordering of the dimensions in the input, specifically where the *channels* dimension is in the data. \n",
        "\n",
        "The possible values that the parameter can take are *channels_first* or *channels_last*. For two-dimensional images, *channels_last* assumes **(batch, height, width, channels)** while  *channels_first* assumes **(batch, channels, height, width)**.\n",
        "\n",
        "More information about the Keras backend and the significance of *image_data_format* can be found here: \n",
        "\n",
        "1.https://keras.io/backend/\n",
        "\n",
        "2.https://www.codesofinterest.com/2017/09/keras-image-data-format.html\n",
        "\n",
        "**NOTE**: For Python < 3.5, *set_image_dim_ordering* would be the parameter to specify the data format convention. That is,\n",
        "```\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('th')\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "PHRRy_GRinli",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use astype() to change the values to floating-point so that we can get \n",
        "# decimal points after division (see Section 3b. Normalizing the data)\n",
        "from keras import backend as K\n",
        "if K.image_data_format() == 'channels_last':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, 28, 28).astype('float32')\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, 28, 28).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZC8-0krjJyyT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## b. Normalizing the data features\n",
        "\n",
        "It is always a good idea to normalize the pixel values. We will normalize the RGB codes (0 through 255) by dividing them by the max RGB value. "
      ]
    },
    {
      "metadata": {
        "id": "kgMFUeq5J1qS",
        "colab_type": "code",
        "outputId": "427868f7-ad54-45c5-db8a-323ff42be1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13804
        }
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train/ 255\n",
        "x_test = x_test/ 255\n",
        "  \n",
        "print(x_test[0:1])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.32941177]\n",
            "   [0.7254902 ]\n",
            "   [0.62352943]\n",
            "   [0.5921569 ]\n",
            "   [0.23529412]\n",
            "   [0.14117648]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.87058824]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.94509804]\n",
            "   [0.7764706 ]\n",
            "   [0.7764706 ]\n",
            "   [0.7764706 ]\n",
            "   [0.7764706 ]\n",
            "   [0.7764706 ]\n",
            "   [0.7764706 ]\n",
            "   [0.7764706 ]\n",
            "   [0.7764706 ]\n",
            "   [0.6666667 ]\n",
            "   [0.20392157]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.2627451 ]\n",
            "   [0.44705883]\n",
            "   [0.28235295]\n",
            "   [0.44705883]\n",
            "   [0.6392157 ]\n",
            "   [0.8901961 ]\n",
            "   [0.99607843]\n",
            "   [0.88235295]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.98039216]\n",
            "   [0.8980392 ]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.54901963]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.06666667]\n",
            "   [0.25882354]\n",
            "   [0.05490196]\n",
            "   [0.2627451 ]\n",
            "   [0.2627451 ]\n",
            "   [0.2627451 ]\n",
            "   [0.23137255]\n",
            "   [0.08235294]\n",
            "   [0.9254902 ]\n",
            "   [0.99607843]\n",
            "   [0.41568628]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.3254902 ]\n",
            "   [0.99215686]\n",
            "   [0.81960785]\n",
            "   [0.07058824]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.08627451]\n",
            "   [0.9137255 ]\n",
            "   [1.        ]\n",
            "   [0.3254902 ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.5058824 ]\n",
            "   [0.99607843]\n",
            "   [0.93333334]\n",
            "   [0.17254902]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.23137255]\n",
            "   [0.9764706 ]\n",
            "   [0.99607843]\n",
            "   [0.24313726]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.52156866]\n",
            "   [0.99607843]\n",
            "   [0.73333335]\n",
            "   [0.01960784]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.03529412]\n",
            "   [0.8039216 ]\n",
            "   [0.972549  ]\n",
            "   [0.22745098]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.49411765]\n",
            "   [0.99607843]\n",
            "   [0.7137255 ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.29411766]\n",
            "   [0.9843137 ]\n",
            "   [0.9411765 ]\n",
            "   [0.22352941]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.07450981]\n",
            "   [0.8666667 ]\n",
            "   [0.99607843]\n",
            "   [0.6509804 ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.01176471]\n",
            "   [0.79607844]\n",
            "   [0.99607843]\n",
            "   [0.85882354]\n",
            "   [0.13725491]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.14901961]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.3019608 ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.12156863]\n",
            "   [0.8784314 ]\n",
            "   [0.99607843]\n",
            "   [0.4509804 ]\n",
            "   [0.00392157]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.52156866]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.20392157]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.23921569]\n",
            "   [0.9490196 ]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.20392157]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.4745098 ]\n",
            "   [0.99607843]\n",
            "   [0.99607843]\n",
            "   [0.85882354]\n",
            "   [0.15686275]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.4745098 ]\n",
            "   [0.99607843]\n",
            "   [0.8117647 ]\n",
            "   [0.07058824]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XlgtRr3QJ20f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## c. Binarize data targets\n",
        "\n",
        "The MNIST dataset consists of images as inputs and the corresponding digit, represented by the image, as targets. The labels range from 0 through 9. Since machines understand only zeroes and ones, we need to represent these labels accordingly before we training our network. To achieve this, we can use the technique of **one-hot encoding**, which converts the label vectors to binary matrices.\n",
        "\n",
        "The *np_utils.to_categorical( )* method in **keras.utils** can be used to generate the one-hot encoding for the target arrays; both train and test sets."
      ]
    },
    {
      "metadata": {
        "id": "arm6Vu3XJ3by",
        "colab_type": "code",
        "outputId": "e014ca67-de6e-455f-c872-a2d3bf8ade3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "#classes = 10 \n",
        "#encode label 6 => [0 0 0 0 0 0 1 0 0 0]\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "print(y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FihyzQNonUmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 4. Model Building\n",
        "\n",
        "The remainder of the code contains the most important part of the learning process: model building, training and evaluation.\n",
        "\n",
        "0.   Choose hyperparameters for your model\n",
        "1.   Build the Convolutional Neural Network\n",
        "2.   Define what optimizer to use\n",
        "3.   Define the loss function\n",
        "4.   What metrics will be used to evaluate the performance\n",
        "5.   Train a model on the training data\n",
        "6.   Validate the model on the validation data\n",
        "7.   Test the learned model on the test data"
      ]
    },
    {
      "metadata": {
        "id": "ouTcyJF_BAEZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0. Define model hyperparameters\n",
        "\n",
        "A *model hyperparameter* refers to an external configuration of the model which are usually specified by the model programmer using heuristics. There does not exist a hard fas rule to find the best value for a hyperparameter. We rely on searching for this best value by trial and error approach via experiments. When you tune a machine learning algorithm (in other words, the resulting model), it means that we tune the hyperparameters of the model, with the objective of obtaining the most appropriate prediction outcomes.\n",
        "\n",
        "The following (immediate) code defines what hyperparameters are chosen for this task and we initialize them with some values."
      ]
    },
    {
      "metadata": {
        "id": "jHfBoX8-6ixl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 200\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0zzSmeC27Gsc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Build the Convolutional Neural Network\n",
        "\n",
        "Our simple Convolutional Neural Network (CNN) will consist of two ***convolutional*** layers, followed by a ***pooling*** layer. A ***dropout*** layer will be added after the pooling layer. Then we add a ***fully-connected*** layer with 128 hidden units (with *RELU* as the activation function). Finally, we add another ***fully-connected*** layer (output layer) with 10 hidden units and *softmax* as the activation function.\n",
        "\n",
        "The code in the following cell implements the architecture CNN described above. "
      ]
    },
    {
      "metadata": {
        "id": "eaicoW7U6i8t",
        "colab_type": "code",
        "outputId": "8237af51-8ec2-4070-b709-3939e302d9ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "''' YOUR CODE GOES HERE '''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5), input_shape=(28,28, 1), activation='relu'))\n",
        "model.add(Conv2D(64, (5,5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 32)        832       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 20, 20, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               819328    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 872,714\n",
            "Trainable params: 872,714\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JE61-ilU7YNF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Select the optimizer, loss function and metrics to evaluate the model\n",
        "\n",
        "A machine learning model is all about optimizing the variables of the model, based on some constraints that results in an objective function. The objective function is typically minimizing some error measure or maximizing some utility function. More specifically, optimization is about how data can be effectively used, how to avoid local minima and search for a good solution. In order to achieve this, we make use of optimization. While there are many such optimization algorithms, we will be using **Adam's optimization function**.\n",
        "\n",
        "A loss function is a simple method that evaluates how well an algorithm models the dataset. The loss function is what tells you if your algorithm is making any improvements than before. We will use **categorical_crossentropy** as the loss function, since our data has labels ranging from 0 through 9 (multi-label classification problem).\n",
        "\n",
        "Metrics are used to determine how performance of various machine learning algorithms can be measured and compared. While there are numerous metrics that could be used, for this task, we will use **accuracy** as our performance metric.\n",
        "\n",
        "The *compile( )* method is used to specify what optimizer should be used by our model, along with the loss function and the evaluation metric. See the documentation to know about other method parameters that you can use [here](https://keras.io/models/model/#compile)"
      ]
    },
    {
      "metadata": {
        "id": "GltjkWU96jH3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIHkhVaT7vCt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training\n",
        "\n",
        "The *fit( )* method in Keras is used to train the model on a given data set (here, *x_train* and *y_train*). The method also takes a parameter *validation_data*, which allows us to specify the validation set on which the model should be evaluated. To know more about other parameters to the *fit( )* method, refer to Keras documentation [here](https://keras.io/models/model/#fit)"
      ]
    },
    {
      "metadata": {
        "id": "_WiltHRO6jQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "67786652-591e-4fdd-f496-07130892f7cb"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "obWcN_mq8sLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation\n",
        "\n",
        "Once we have agreed on the best model (on the validation data), the next step would be to test the model on new, unseen data. The *evaluate( )* method in Keras is used to apply the model on the test set and report the model performance on new, unseen data. Note that, this test set has not been used (and should NOT be!) used during model training. [Here](https://keras.io/models/model/#evaluate) is the documentation about the *evaluate( )* method, in case you would like to know more about the parameters that can be used in this method.\n",
        "\n",
        "There also exists another method called *predict( )* in Keras, which is used to only generate the output predictions. The difference between *evaluate( )* and *predict( )* is that the former returns the loss value and metric while testing the model, but the latter does not. (see [here](https://keras.io/models/model/#predict))"
      ]
    },
    {
      "metadata": {
        "id": "aHJvfG4T6jaL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "print('Test Loss:', scores[0])\n",
        "print('Test Accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}